{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# =========================\n","# Apenas 1 Prompt\n","# =========================\n","\n","NOME_ARQUIVO_EXCEL = \"LICITA_plus_dataset_template.xlsx\"\n","PASTA_DRIVE = \"My Drive/üìÅ FACULDADE/2025 - 8¬∫ Per√≠odo/Trabalho de Conclus√£o de Curso II\"\n","\n","OPENAI_API_KEY = \"\"      # use Secrets (google.colab.userdata)\n","ANTHROPIC_API_KEY = \"\"   # use Secrets\n","DEEPSEEK_API_KEY = \"\"    # use Secrets\n","\n","# Execu√ß√µes para m√©dia\n","N_EXECUCOES_PRECO = 10\n","SLEEP_ENTRE_CHAMADAS = 1.0  # seg\n","\n","# Logging / Auditoria\n","VERBOSO = True\n","LOG_RAW_RESPOSTA = False     # True = inclui a resposta bruta no TXT\n","LOG_SALVAR_TXT = True        # True = salva o arquivo de logs no final\n","LOG_REGISTROS = []           # buffer de logs\n","\n","# =========================\n","# Instala√ß√£o de deps\n","# =========================\n","\n","import subprocess, sys\n","def instalar():\n","    deps = [\n","        \"openai>=1.3.0\",\n","        \"anthropic>=0.18.0\",\n","        \"pandas>=1.5.0\",\n","        \"openpyxl>=3.1.0\",\n","        \"numpy>=1.23.0\",\n","        \"tqdm>=4.66.0\"\n","    ]\n","    for dep in deps:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", dep, \"-q\"])\n","instalar()\n","\n","# =========================\n","# Imports\n","# =========================\n","\n","import pandas as pd\n","import numpy as np\n","import openai\n","import anthropic\n","import time\n","import os\n","import re\n","from datetime import datetime\n","from tqdm.auto import tqdm\n","\n","# =========================\n","# APIs\n","# =========================\n","\n","from google.colab import userdata\n","\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY') or OPENAI_API_KEY\n","ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY') or ANTHROPIC_API_KEY\n","DEEPSEEK_API_KEY = userdata.get('DEEPSEEK_API_KEY') or DEEPSEEK_API_KEY\n","\n","openai.api_key = OPENAI_API_KEY\n","cliente_anthropic = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n","cliente_deepseek = openai.OpenAI(api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\")\n","\n","print(\"[OK] APIs configuradas\", flush=True)\n","\n","# =========================\n","# Google Drive\n","# =========================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","print(\"[OK] Drive montado\", flush=True)\n","\n","# =========================\n","# Utilit√°rios\n","# =========================\n","\n","NUM_REGEX = re.compile(r'(\\d{1,6}(?:[.,]\\d{1,2})?)')\n","\n","def extrair_preco(texto):\n","    if not texto:\n","        return None\n","    m = NUM_REGEX.findall(str(texto))\n","    if not m:\n","        return None\n","    try:\n","        v = float(m[0].replace(',', '.'))\n","        if 1 <= v <= 50000:\n","            return round(v, 2)\n","    except:\n","        return None\n","    return None\n","\n","def logf(msg):\n","    # grava no buffer e deixa a sa√≠da enxuta no colab\n","    LOG_REGISTROS.append(msg)\n","\n","def salvar_logs_txt(pasta_drive):\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    caminho = f\"/content/drive/{pasta_drive}/logs_execucao_{ts}.txt\"\n","    with open(caminho, \"w\", encoding=\"utf-8\") as f:\n","        f.write(\"\\n\".join(LOG_REGISTROS))\n","        f.write(\"\\n\")\n","    print(f\"[OK] Log salvo: {os.path.basename(caminho)}\", flush=True)\n","    return caminho\n","\n","# =========================\n","# Planilha\n","# =========================\n","\n","def carregar_planilha(nome_arquivo, pasta_drive):\n","    caminho = f\"/content/drive/{pasta_drive}/{nome_arquivo}\"\n","    try:\n","        df = pd.read_excel(caminho, engine='openpyxl')\n","        if 'descricao_item_bruta' not in df.columns:\n","            raise ValueError(\"Coluna 'descricao_item_bruta' n√£o encontrada.\")\n","        print(f\"[OK] Planilha: {len(df)} linhas | descri√ß√µes v√°lidas: {df['descricao_item_bruta'].notna().sum()}\", flush=True)\n","        return df, caminho\n","    except Exception as e:\n","        print(f\"[ERRO] Falha ao carregar planilha: {e}\", flush=True)\n","        raise\n","\n","def salvar_relatorio_final(df, pasta_drive):\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    destino = f\"/content/drive/{pasta_drive}/relatorio_final_{ts}.xlsx\"\n","    try:\n","        df.to_excel(destino, index=False, engine='openpyxl')\n","        print(f\"[OK] Relat√≥rio salvo: {os.path.basename(destino)}\", flush=True)\n","        return True, destino\n","    except Exception as e:\n","        print(f\"[ERRO] Falha ao salvar relat√≥rio: {e}\", flush=True)\n","        return False, None\n","\n","# =========================\n","# Chamadas de modelos\n","# =========================\n","\n","def consultar_gpt4o(prompt, max_tokens=200, temperatura=0.1):\n","    for tent in range(1, 4):\n","        try:\n","            rsp = openai.chat.completions.create(\n","                model=\"gpt-4o-mini\",\n","                messages=[\n","                    {\"role\": \"system\", \"content\": \"Voc√™ √© um auditor brasileiro experiente.\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                max_tokens=max_tokens,\n","                temperature=temperatura\n","            )\n","            return rsp.choices[0].message.content.strip()\n","        except Exception as e:\n","            print(f\"[WARN] GPT tentativa {tent} falhou: {str(e)[:80]}\", flush=True)\n","            time.sleep(3)\n","    return None\n","\n","def consultar_claude(prompt, max_tokens=200, temperatura=0.1):\n","    for tent in range(1, 4):\n","        try:\n","            msg = cliente_anthropic.messages.create(\n","                model=\"claude-sonnet-4-5-20250929\",\n","                max_tokens=max_tokens,\n","                temperature=temperatura,\n","                messages=[{\"role\": \"user\", \"content\": prompt}]\n","            )\n","            return msg.content[0].text.strip()\n","        except Exception as e:\n","            print(f\"[WARN] Claude tentativa {tent} falhou: {str(e)[:80]}\", flush=True)\n","            time.sleep(3)\n","    return None\n","\n","def consultar_deepseek(prompt, max_tokens=200, temperatura=0.1):\n","    for tent in range(1, 4):\n","        try:\n","            rsp = cliente_deepseek.chat.completions.create(\n","                model=\"deepseek-chat\",\n","                messages=[\n","                    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                max_tokens=max_tokens,\n","                temperature=temperatura\n","            )\n","            return rsp.choices[0].message.content.strip()\n","        except Exception as e:\n","            s = str(e)\n","            if \"Insufficient Balance\" in s or \"402\" in s:\n","                print(f\"[WARN] DeepSeek sem saldo (tentativa {tent})\", flush=True)\n","                return None\n","            print(f\"[WARN] DeepSeek tentativa {tent} falhou: {s[:80]}\", flush=True)\n","            time.sleep(3)\n","    return None\n","\n","# =========================\n","# Execu√ß√£o repetida para m√©dia (com tqdm + log)\n","# =========================\n","\n","def rodar_precos_em_lote(func_modelo, prompt_preco, n_execucoes=N_EXECUCOES_PRECO, sleep=SLEEP_ENTRE_CHAMADAS, desc_barra=\"\", position=0, item_id=\"\", modelo_nome=\"\", produto=\"\"):\n","    valores = []\n","    bar = tqdm(total=n_execucoes, desc=desc_barra, leave=False, position=position, dynamic_ncols=True)\n","    try:\n","        for i in range(1, n_execucoes + 1):\n","            txt = func_modelo(prompt_preco, max_tokens=20, temperatura=0.05)\n","            raw = repr(txt) if LOG_RAW_RESPOSTA else \"<raw oculto>\"\n","            v = extrair_preco(txt)\n","            if v is not None:\n","                valores.append(v)\n","            media_parcial = float(np.mean(valores)) if valores else None\n","            std_parcial = float(np.std(valores, ddof=1)) if len(valores) > 1 else 0.0 if valores else None\n","            n_ok = len(valores)\n","\n","            # Log linha-a-linha de auditoria\n","            logf(f\"{item_id} | {modelo_nome} | produto={produto} | exec={i}/{n_execucoes} | preco={v if v is not None else 'INVALIDO'} | media_parcial={media_parcial if media_parcial is not None else 'NA'} | std_parcial={std_parcial if std_parcial is not None else 'NA'} | n_ok={n_ok} | raw={raw}\")\n","\n","            postfix = {}\n","            if media_parcial is not None:\n","                postfix.update(dict(media=f\"{media_parcial:.2f}\", std=f\"{std_parcial:.2f}\", n=n_ok))\n","            bar.set_postfix(postfix)\n","            bar.update(1)\n","            time.sleep(sleep)\n","    finally:\n","        bar.close()\n","\n","    if len(valores) == 0:\n","        return None, None, 0\n","    media = float(np.mean(valores))\n","    std = float(np.std(valores, ddof=1)) if len(valores) > 1 else 0.0\n","    # Log resumo final do modelo\n","    logf(f\"{item_id} | {modelo_nome} | FINAL | media={media:.2f} | std={std:.2f} | n={len(valores)}\")\n","    return media, std, len(valores)\n","\n","# =========================\n","# Processo por item (com auditoria)\n","# =========================\n","\n","def processar_item_3modelos(descricao_bruta, index, base_position=1):\n","    r = {\n","        'produto_identificado_modelo_01': None,\n","        'preco_estimado_modelo_01': None,\n","        'preco_std_modelo_01': None,\n","        'preco_n_modelo_01': 0,\n","\n","        'produto_identificado_modelo_02': None,\n","        'preco_estimado_modelo_02': None,\n","        'preco_std_modelo_02': None,\n","        'preco_n_modelo_02': 0,\n","\n","        'produto_identificado_modelo_03': None,\n","        'preco_estimado_modelo_03': None,\n","        'preco_std_modelo_03': None,\n","        'preco_n_modelo_03': 0,\n","\n","        'prompt_id': f\"3MODELS_{index:04d}\",\n","        'status_analise': 'PENDENTE',\n","        'observacoes': None\n","    }\n","\n","    if pd.isna(descricao_bruta) or str(descricao_bruta).strip() == '':\n","        r['status_analise'] = 'ERRO'\n","        r['observacoes'] = 'Descri√ß√£o vazia'\n","        logf(f\"ITEM {index+1} | {r['prompt_id']} | descricao=<vazia> | STATUS=ERRO\")\n","        return r\n","\n","    descricao = str(descricao_bruta).strip()\n","    logf(f\"=== ITEM {index+1} | {r['prompt_id']} ===\")\n","    logf(f\"descricao_item_bruta: {descricao}\")\n","\n","    prompt_identificacao = (\n","        \"Voc√™ ver√° a descri√ß√£o de um item de licita√ß√£o.\\n\\n\"\n","        f'DESCRI√á√ÉO:\\n\"{descricao}\"\\n\\n'\n","        \"TAREFA:\\nIdentifique o produto REAL correspondente no varejo brasileiro. \"\n","        \"Inclua marca e modelo quando poss√≠vel.\\n\\n\"\n","        \"Responda APENAS com o nome do produto:\"\n","    )\n","\n","    prompt_preco_template = (\n","        \"Considere o produto abaixo e informe um pre√ßo m√©dio unit√°rio no varejo do Brasil.\\n\\n\"\n","        \"Produto: {produto}\\n\\n\"\n","        \"Regras:\\n\"\n","        \"- Pre√ßo de 1 unidade (n√£o atacado)\\n\"\n","        \"- Pense em lojas brasileiras (Amazon, Magalu, Kabum, Kalunga)\\n\"\n","        \"- Responda APENAS com o valor num√©rico (ex: 2899.90)\\n\\n\"\n","        \"Pre√ßo:\"\n","    )\n","\n","    obs = []\n","\n","    # ---- GPT-4o-mini ----\n","    try:\n","        if VERBOSO: print(\"[GPT-4o-mini] identificando produto...\", flush=True)\n","        prod = consultar_gpt4o(prompt_identificacao, max_tokens=80, temperatura=0.1)\n","        if prod:\n","            prod = prod.strip().replace('\"', '').replace(\"'\", \"\")\n","            r['produto_identificado_modelo_01'] = prod\n","            logf(f\"{r['prompt_id']} | GPT-4o-mini | produto_identificado={prod}\")\n","            if VERBOSO: print(f\"[GPT-4o-mini] produto: {prod}\", flush=True)\n","            prompt_preco = prompt_preco_template.format(produto=prod)\n","            media, desvio, n_ok = rodar_precos_em_lote(\n","                consultar_gpt4o, prompt_preco,\n","                n_execucoes=N_EXECUCOES_PRECO,\n","                sleep=SLEEP_ENTRE_CHAMADAS,\n","                desc_barra=f\"[GPT-4o-mini] pre√ßos\",\n","                position=base_position,\n","                item_id=r['prompt_id'],\n","                modelo_nome=\"GPT-4o-mini\",\n","                produto=prod\n","            )\n","            if media is not None:\n","                r['preco_estimado_modelo_01'] = round(media, 2)\n","                r['preco_std_modelo_01'] = round(desvio, 2)\n","                r['preco_n_modelo_01'] = int(n_ok)\n","        else:\n","            obs.append(\"GPT: falha\")\n","            logf(f\"{r['prompt_id']} | GPT-4o-mini | FALHA_IDENTIFICACAO\")\n","    except Exception as e:\n","        obs.append(\"GPT: erro\")\n","        logf(f\"{r['prompt_id']} | GPT-4o-mini | ERRO | {repr(e)}\")\n","\n","    # ---- Claude ----\n","    try:\n","        if VERBOSO: print(\"[Claude] identificando produto...\", flush=True)\n","        prod = consultar_claude(prompt_identificacao, max_tokens=80, temperatura=0.1)\n","        if prod:\n","            prod = prod.strip().replace('\"', '').replace(\"'\", \"\")\n","            r['produto_identificado_modelo_02'] = prod\n","            logf(f\"{r['prompt_id']} | Claude | produto_identificado={prod}\")\n","            if VERBOSO: print(f\"[Claude] produto: {prod}\", flush=True)\n","            prompt_preco = prompt_preco_template.format(produto=prod)\n","            media, desvio, n_ok = rodar_precos_em_lote(\n","                consultar_claude, prompt_preco,\n","                n_execucoes=N_EXECUCOES_PRECO,\n","                sleep=SLEEP_ENTRE_CHAMADAS,\n","                desc_barra=f\"[Claude] pre√ßos\",\n","                position=base_position+1,\n","                item_id=r['prompt_id'],\n","                modelo_nome=\"Claude\",\n","                produto=prod\n","            )\n","            if media is not None:\n","                r['preco_estimado_modelo_02'] = round(media, 2)\n","                r['preco_std_modelo_02'] = round(desvio, 2)\n","                r['preco_n_modelo_02'] = int(n_ok)\n","        else:\n","            obs.append(\"Claude: falha\")\n","            logf(f\"{r['prompt_id']} | Claude | FALHA_IDENTIFICACAO\")\n","    except Exception as e:\n","        obs.append(\"Claude: erro\")\n","        logf(f\"{r['prompt_id']} | Claude | ERRO | {repr(e)}\")\n","\n","    # ---- DeepSeek ----\n","    try:\n","        if VERBOSO: print(\"[DeepSeek] identificando produto...\", flush=True)\n","        prod = consultar_deepseek(prompt_identificacao, max_tokens=80, temperatura=0.1)\n","        if prod:\n","            prod = prod.strip().replace('\"', '').replace(\"'\", \"\")\n","            r['produto_identificado_modelo_03'] = prod\n","            logf(f\"{r['prompt_id']} | DeepSeek | produto_identificado={prod}\")\n","            if VERBOSO: print(f\"[DeepSeek] produto: {prod}\", flush=True)\n","            prompt_preco = prompt_preco_template.format(produto=prod)\n","            media, desvio, n_ok = rodar_precos_em_lote(\n","                consultar_deepseek, prompt_preco,\n","                n_execucoes=N_EXECUCOES_PRECO,\n","                sleep=SLEEP_ENTRE_CHAMADAS,\n","                desc_barra=f\"[DeepSeek] pre√ßos\",\n","                position=base_position+2,\n","                item_id=r['prompt_id'],\n","                modelo_nome=\"DeepSeek\",\n","                produto=prod\n","            )\n","            if media is not None:\n","                r['preco_estimado_modelo_03'] = round(media, 2)\n","                r['preco_std_modelo_03'] = round(desvio, 2)\n","                r['preco_n_modelo_03'] = int(n_ok)\n","        else:\n","            obs.append(\"DeepSeek: sem saldo ou falha\")\n","            logf(f\"{r['prompt_id']} | DeepSeek | FALHA_IDENTIFICACAO\")\n","    except Exception as e:\n","        obs.append(\"DeepSeek: erro\")\n","        logf(f\"{r['prompt_id']} | DeepSeek | ERRO | {repr(e)}\")\n","\n","    r['status_analise'] = 'PROCESSADO'\n","    if obs:\n","        r['observacoes'] = \"; \".join(obs)\n","        logf(f\"{r['prompt_id']} | OBSERVACOES: {r['observacoes']}\")\n","    logf(f\"=== FIM ITEM {index+1} | {r['prompt_id']} ===\")\n","    return r\n","\n","# =========================\n","# Processar planilha (barra geral)\n","# =========================\n","\n","def processar_planilha(df, limite_linhas=None):\n","    n = limite_linhas if limite_linhas else len(df)\n","    n = min(n, len(df))\n","    print(f\"[INFO] Processando {n} itens (m√©dia de {N_EXECUCOES_PRECO} execu√ß√µes por modelo)\", flush=True)\n","\n","    colunas = {\n","        'produto_identificado_modelo_01': 'object',\n","        'preco_estimado_modelo_01': 'float64',\n","        'preco_std_modelo_01': 'float64',\n","        'preco_n_modelo_01': 'int64',\n","\n","        'produto_identificado_modelo_02': 'object',\n","        'preco_estimado_modelo_02': 'float64',\n","        'preco_std_modelo_02': 'float64',\n","        'preco_n_modelo_02': 'int64',\n","\n","        'produto_identificado_modelo_03': 'object',\n","        'preco_estimado_modelo_03': 'float64',\n","        'preco_std_modelo_03': 'float64',\n","        'preco_n_modelo_03': 'int64',\n","\n","        'prompt_id': 'object',\n","        'status_analise': 'object',\n","        'observacoes': 'object'\n","    }\n","    for col, dtype in colunas.items():\n","        if col not in df.columns:\n","            df[col] = None\n","    for col, dtype in colunas.items():\n","        try:\n","            df[col] = df[col].astype(dtype)\n","        except Exception:\n","            pass\n","\n","    with tqdm(total=n, desc=\"[Itens]\", position=0, dynamic_ncols=True) as bar_itens:\n","        processados = 0\n","        for idx in range(n):\n","            if pd.notna(df.at[idx, 'status_analise']) and df.at[idx, 'status_analise'] == 'PROCESSADO':\n","                bar_itens.set_postfix(dict(status=\"skip\"))\n","                bar_itens.update(1)\n","                continue\n","\n","            desc = str(df.iloc[idx].get('descricao_item_bruta', ''))[:80].replace(\"\\n\", \" \")\n","            logf(f\"----- INICIO PROCESSO ITEM {idx+1}/{n} -----\")\n","            logf(f\"descricao_preview: {desc}\")\n","            bar_itens.set_postfix(dict(item=f\"{idx+1}/{n}\"))\n","            r = processar_item_3modelos(df.iloc[idx].get('descricao_item_bruta', ''), idx, base_position=1)\n","            for k, v in r.items():\n","                df.at[idx, k] = v\n","            processados += 1\n","            bar_itens.update(1)\n","            logf(f\"----- FIM PROCESSO ITEM {idx+1}/{n} -----\")\n","\n","    print(f\"[OK] Conclu√≠do. Itens processados: {processados}\", flush=True)\n","    return df\n","\n","# =========================\n","# Relat√≥rio\n","# =========================\n","\n","def gerar_relatorio(df):\n","    df_proc = df[df['status_analise'] == 'PROCESSADO']\n","    if len(df_proc) == 0:\n","        print(\"[INFO] Nenhum item processado\", flush=True)\n","        return\n","\n","    print(\"[RELAT√ìRIO] Resumo de pre√ßos por modelo (m√©dia ¬± desvio, n v√°lidos)\", flush=True)\n","    modelos = [\n","        (1, \"GPT-4o-mini\"),\n","        (2, \"Claude Sonnet 4.5\"),\n","        (3, \"DeepSeek\")\n","    ]\n","    for num, nome in modelos:\n","        col_preco = f'preco_estimado_modelo_0{num}'\n","        col_std   = f'preco_std_modelo_0{num}'\n","        col_n     = f'preco_n_modelo_0{num}'\n","        dfp = df_proc[df_proc[col_preco].notna()]\n","        if len(dfp) > 0:\n","            precos = dfp[col_preco].astype(float)\n","            print(f\"- {nome}: linhas com pre√ßo={len(dfp)}/{len(df_proc)} | \"\n","                  f\"m√©dia_global={precos.mean():.2f} | mediana={precos.median():.2f} | \"\n","                  f\"min={precos.min():.2f} | max={precos.max():.2f}\", flush=True)\n","        else:\n","            print(f\"- {nome}: nenhum pre√ßo\", flush=True)\n","\n","# =========================\n","# Execu√ß√£o\n","# =========================\n","\n","print(\"[START] LICITA+ v7.6 - auditoria TXT + tqdm\", flush=True)\n","df_licitacoes, _ = carregar_planilha(NOME_ARQUIVO_EXCEL, PASTA_DRIVE)\n","\n","df_atualizado = processar_planilha(\n","    df_licitacoes.copy(),\n","    limite_linhas= None  # ajuste para None para processar tudo\n",")\n","\n","ok, _ = salvar_relatorio_final(df_atualizado, PASTA_DRIVE)\n","gerar_relatorio(df_atualizado)\n","\n","if LOG_SALVAR_TXT:\n","    caminho_logs = salvar_logs_txt(PASTA_DRIVE)\n","\n","print(\"[END]\", flush=True)\n"],"metadata":{"id":"BbNQHBE-TvUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Configura√ß√µes\n","# =========================\n","\n","NOME_ARQUIVO_EXCEL = \"LICITA_plus_dataset_template.xlsx\"\n","PASTA_DRIVE = \"My Drive/üìÅ FACULDADE/2025 - 8¬∫ Per√≠odo/Trabalho de Conclus√£o de Curso II\"\n","\n","OPENAI_API_KEY = \"\"      # use Secrets (google.colab.userdata)\n","ANTHROPIC_API_KEY = \"\"   # use Secrets\n","DEEPSEEK_API_KEY = \"\"    # use Secrets\n","\n","# Configura√ß√£o para TESTE - apenas 5 primeiros produtos\n","LIMITE_TESTE = 5\n","SLEEP_ENTRE_CHAMADAS = 1.0  # seg\n","\n","# Logging / Auditoria\n","VERBOSO = True\n","LOG_SALVAR_TXT = True\n","LOG_REGISTROS = []\n","\n","# =========================\n","# Instala√ß√£o de deps\n","# =========================\n","\n","import subprocess, sys\n","def instalar():\n","    deps = [\n","        \"openai>=1.3.0\",\n","        \"anthropic>=0.18.0\",\n","        \"pandas>=1.5.0\",\n","        \"openpyxl>=3.1.0\",\n","        \"numpy>=1.23.0\",\n","        \"tqdm>=4.66.0\"\n","    ]\n","    for dep in deps:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", dep, \"-q\"])\n","instalar()\n","\n","# =========================\n","# Imports\n","# =========================\n","\n","import pandas as pd\n","import numpy as np\n","import openai\n","import anthropic\n","import time\n","import os\n","import re\n","from datetime import datetime\n","from tqdm.auto import tqdm\n","\n","# =========================\n","# APIs\n","# =========================\n","\n","from google.colab import userdata\n","\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY') or OPENAI_API_KEY\n","ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY') or ANTHROPIC_API_KEY\n","DEEPSEEK_API_KEY = userdata.get('DEEPSEEK_API_KEY') or DEEPSEEK_API_KEY\n","\n","openai.api_key = OPENAI_API_KEY\n","cliente_anthropic = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n","cliente_deepseek = openai.OpenAI(api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\")\n","\n","print(\"[OK] APIs configuradas\", flush=True)\n","\n","# =========================\n","# Google Drive\n","# =========================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","print(\"[OK] Drive montado\", flush=True)\n","\n","# =========================\n","# 5 PROMPTS PARA IDENTIFICA√á√ÉO DE PRODUTO\n","# =========================\n","\n","PROMPTS_IDENTIFICACAO = {\n","    \"PromptID_1\": \"\"\"Identifique o produto principal desta descri√ß√£o de licita√ß√£o e responda APENAS com o nome do produto (sem quantidades, marcas ou detalhes):\n","\n","Descri√ß√£o: {descricao}\n","\n","Produto:\"\"\",\n","\n","    \"PromptID_2\": \"\"\"Voc√™ √© um auditor especializado em licita√ß√µes. Analise a descri√ß√£o abaixo e extraia o nome do produto/item principal, respondendo de forma objetiva e concisa:\n","\n","{descricao}\n","\n","Nome do produto:\"\"\",\n","\n","    \"PromptID_3\": \"\"\"Com base na descri√ß√£o de licita√ß√£o a seguir, qual √© o produto/item sendo licitado? Responda apenas com o nome do produto principal.\n","\n","Descri√ß√£o da licita√ß√£o:\n","{descricao}\n","\n","Resposta:\"\"\",\n","\n","    \"PromptID_4\": \"\"\"Analise a descri√ß√£o e identifique o produto:\n","\n","{descricao}\n","\n","Qual produto est√° sendo descrito? (responda somente com o nome)\"\"\",\n","\n","    \"PromptID_5\": \"\"\"Extraia o produto principal desta descri√ß√£o de compra p√∫blica:\n","\n","DESCRI√á√ÉO: {descricao}\n","\n","PRODUTO IDENTIFICADO:\"\"\"\n","}\n","\n","# =========================\n","# 5 PROMPTS PARA ESTIMATIVA DE PRE√áO\n","# =========================\n","\n","PROMPTS_PRECO = {\n","    \"PromptPreco_1\": \"\"\"Com base no mercado brasileiro, estime o pre√ßo unit√°rio m√©dio em reais (R$) para o produto:\n","\n","{produto}\n","\n","Retorne apenas o valor num√©rico (ex: 125.50). Se n√£o souber, retorne \"N/A\".\n","\n","Pre√ßo estimado:\"\"\",\n","\n","    \"PromptPreco_2\": \"\"\"Voc√™ √© um especialista em precifica√ß√£o de produtos no Brasil. Qual seria o pre√ßo m√©dio de mercado para:\n","\n","Produto: {produto}\n","\n","Responda apenas com o valor em reais (exemplo: 89.90):\"\"\",\n","\n","    \"PromptPreco_3\": \"\"\"Estime o valor unit√°rio t√≠pico no mercado brasileiro para o seguinte produto:\n","\n","{produto}\n","\n","Valor estimado (somente n√∫mero):\"\"\",\n","\n","    \"PromptPreco_4\": \"\"\"Considerando o mercado nacional, forne√ßa uma estimativa de pre√ßo para:\n","\n","{produto}\n","\n","Pre√ßo em R$ (apenas n√∫mero):\"\"\",\n","\n","    \"PromptPreco_5\": \"\"\"Qual o pre√ßo m√©dio de mercado no Brasil para este produto?\n","\n","PRODUTO: {produto}\n","\n","PRE√áO (R$):\"\"\"\n","}\n","\n","# =========================\n","# Utilit√°rios\n","# =========================\n","\n","NUM_REGEX = re.compile(r'(\\d{1,6}(?:[.,]\\d{1,2})?)')\n","\n","def extrair_preco(texto):\n","    if not texto:\n","        return None\n","    m = NUM_REGEX.findall(str(texto))\n","    if not m:\n","        return None\n","    try:\n","        v = float(m[0].replace(',', '.'))\n","        if 1 <= v <= 50000:\n","            return round(v, 2)\n","    except:\n","        return None\n","    return None\n","\n","def logf(msg):\n","    LOG_REGISTROS.append(msg)\n","\n","def salvar_logs_txt(pasta_drive):\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    caminho = f\"/content/drive/{pasta_drive}/logs_teste_5produtos_{ts}.txt\"\n","    with open(caminho, \"w\", encoding=\"utf-8\") as f:\n","        f.write(\"\\n\".join(LOG_REGISTROS))\n","        f.write(\"\\n\")\n","    print(f\"[OK] Log salvo: {os.path.basename(caminho)}\", flush=True)\n","    return caminho\n","\n","# =========================\n","# Planilha\n","# =========================\n","\n","def carregar_planilha(nome_arquivo, pasta_drive):\n","    caminho = f\"/content/drive/{pasta_drive}/{nome_arquivo}\"\n","    try:\n","        df = pd.read_excel(caminho, engine='openpyxl')\n","        if 'descricao_item_bruta' not in df.columns:\n","            raise ValueError(\"Coluna 'descricao_item_bruta' n√£o encontrada.\")\n","        print(f\"[OK] Planilha: {len(df)} linhas | descri√ß√µes v√°lidas: {df['descricao_item_bruta'].notna().sum()}\", flush=True)\n","        return df, caminho\n","    except Exception as e:\n","        print(f\"[ERRO] Falha ao carregar planilha: {e}\", flush=True)\n","        raise\n","\n","def salvar_relatorio_final(df, pasta_drive):\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    destino = f\"/content/drive/{pasta_drive}/Resultado_Varios_Prompts_{ts}.xlsx\"\n","    try:\n","        # Criar um Excel writer para formatar melhor\n","        with pd.ExcelWriter(destino, engine='openpyxl') as writer:\n","            # Aba 1: Dados completos\n","            df.to_excel(writer, sheet_name='Dados_Completos', index=False)\n","\n","            # Ajustar largura das colunas\n","            worksheet = writer.sheets['Dados_Completos']\n","            for column in worksheet.columns:\n","                max_length = 0\n","                column_letter = column[0].column_letter\n","                for cell in column:\n","                    try:\n","                        if len(str(cell.value)) > max_length:\n","                            max_length = len(str(cell.value))\n","                    except:\n","                        pass\n","                adjusted_width = min(max_length + 2, 50)\n","                worksheet.column_dimensions[column_letter].width = adjusted_width\n","\n","        print(f\"[OK] Relat√≥rio salvo: {os.path.basename(destino)}\", flush=True)\n","        return True, destino\n","    except Exception as e:\n","        print(f\"[ERRO] Falha ao salvar relat√≥rio: {e}\", flush=True)\n","        return False, None\n","\n","# =========================\n","# Chamadas de modelos\n","# =========================\n","\n","def consultar_gpt4o(prompt, max_tokens=200, temperatura=0.1):\n","    for tent in range(1, 4):\n","        try:\n","            rsp = openai.chat.completions.create(\n","                model=\"gpt-4o-mini\",\n","                messages=[\n","                    {\"role\": \"system\", \"content\": \"Voc√™ √© um auditor brasileiro experiente.\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                max_tokens=max_tokens,\n","                temperature=temperatura\n","            )\n","            return rsp.choices[0].message.content.strip()\n","        except Exception as e:\n","            print(f\"[WARN] GPT tentativa {tent} falhou: {str(e)[:80]}\", flush=True)\n","            time.sleep(3)\n","    return None\n","\n","def consultar_claude(prompt, max_tokens=200, temperatura=0.1):\n","    for tent in range(1, 4):\n","        try:\n","            msg = cliente_anthropic.messages.create(\n","                model=\"claude-sonnet-4-5-20250929\",\n","                max_tokens=max_tokens,\n","                temperature=temperatura,\n","                messages=[{\"role\": \"user\", \"content\": prompt}]\n","            )\n","            return msg.content[0].text.strip()\n","        except Exception as e:\n","            print(f\"[WARN] Claude tentativa {tent} falhou: {str(e)[:80]}\", flush=True)\n","            time.sleep(3)\n","    return None\n","\n","def consultar_deepseek(prompt, max_tokens=200, temperatura=0.1):\n","    for tent in range(1, 4):\n","        try:\n","            rsp = cliente_deepseek.chat.completions.create(\n","                model=\"deepseek-chat\",\n","                messages=[\n","                    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                max_tokens=max_tokens,\n","                temperature=temperatura\n","            )\n","            return rsp.choices[0].message.content.strip()\n","        except Exception as e:\n","            s = str(e)\n","            if \"Insufficient Balance\" in s or \"402\" in s:\n","                print(f\"[WARN] DeepSeek sem saldo (tentativa {tent})\", flush=True)\n","                return None\n","            print(f\"[WARN] DeepSeek tentativa {tent} falhou: {s[:80]}\", flush=True)\n","            time.sleep(3)\n","    return None\n","\n","# =========================\n","# Processar item com TODOS os prompts\n","# =========================\n","\n","def processar_item_completo(descricao, index):\n","    \"\"\"\n","    Processa UM item com:\n","    - 5 prompts de identifica√ß√£o √ó 3 modelos = 15 identifica√ß√µes\n","    - Para cada produto identificado, 5 prompts de pre√ßo √ó 3 modelos = at√© 75 pre√ßos\n","    Total m√°ximo: 90 chamadas por item\n","    \"\"\"\n","    r = {\n","        'item_index': index + 1,\n","        'descricao_original': descricao[:200] + \"...\" if len(descricao) > 200 else descricao,\n","        'status_analise': 'PENDENTE',\n","        'observacoes': ''\n","    }\n","\n","    obs = []\n","    logf(f\"\\n{'='*80}\")\n","    logf(f\"ITEM {index+1} - INICIO DO PROCESSAMENTO\")\n","    logf(f\"{'='*80}\")\n","\n","    modelos_config = [\n","        ('GPT4o', consultar_gpt4o),\n","        ('Claude', consultar_claude),\n","        ('DeepSeek', consultar_deepseek)\n","    ]\n","\n","    # Para cada PROMPT DE IDENTIFICA√á√ÉO\n","    for prompt_id_id, prompt_id_template in PROMPTS_IDENTIFICACAO.items():\n","        logf(f\"\\n--- Testando {prompt_id_id} ---\")\n","\n","        prompt_identificacao = prompt_id_template.format(descricao=descricao[:500])\n","\n","        # Para cada MODELO\n","        for modelo_nome, funcao_modelo in modelos_config:\n","            col_produto = f'Produto_Identificado_{prompt_id_id}_{modelo_nome}'\n","\n","            try:\n","                if VERBOSO:\n","                    print(f\"[{prompt_id_id}] [{modelo_nome}] identificando...\", flush=True)\n","\n","                prod = funcao_modelo(prompt_identificacao, max_tokens=80, temperatura=0.1)\n","\n","                if prod:\n","                    prod = prod.strip().replace('\"', '').replace(\"'\", \"\")\n","                    r[col_produto] = prod\n","                    logf(f\"{prompt_id_id} | {modelo_nome} | Produto identificado: {prod}\")\n","\n","                    # Agora, para cada PROMPT DE PRE√áO\n","                    for prompt_preco_id, prompt_preco_template in PROMPTS_PRECO.items():\n","                        col_preco = f'Preco_{prompt_id_id}_{prompt_preco_id}_{modelo_nome}'\n","\n","                        try:\n","                            prompt_preco = prompt_preco_template.format(produto=prod)\n","                            preco_resp = funcao_modelo(prompt_preco, max_tokens=100, temperatura=0.1)\n","                            preco = extrair_preco(preco_resp)\n","\n","                            if preco:\n","                                r[col_preco] = round(preco, 2)\n","                                logf(f\"  ‚îî‚îÄ {prompt_preco_id} | {modelo_nome} | Pre√ßo: R$ {preco:.2f}\")\n","                            else:\n","                                r[col_preco] = None\n","                                logf(f\"  ‚îî‚îÄ {prompt_preco_id} | {modelo_nome} | Pre√ßo: FALHA\")\n","\n","                            time.sleep(SLEEP_ENTRE_CHAMADAS)\n","\n","                        except Exception as e:\n","                            r[col_preco] = None\n","                            logf(f\"  ‚îî‚îÄ {prompt_preco_id} | {modelo_nome} | ERRO: {repr(e)}\")\n","\n","                else:\n","                    r[col_produto] = None\n","                    obs.append(f\"{prompt_id_id}-{modelo_nome}: falha identifica√ß√£o\")\n","                    logf(f\"{prompt_id_id} | {modelo_nome} | FALHA na identifica√ß√£o\")\n","\n","                time.sleep(SLEEP_ENTRE_CHAMADAS)\n","\n","            except Exception as e:\n","                r[col_produto] = None\n","                obs.append(f\"{prompt_id_id}-{modelo_nome}: erro\")\n","                logf(f\"{prompt_id_id} | {modelo_nome} | ERRO: {repr(e)}\")\n","\n","    r['status_analise'] = 'PROCESSADO'\n","    if obs:\n","        r['observacoes'] = \"; \".join(obs[:10])  # Limitar observa√ß√µes\n","\n","    logf(f\"\\n{'='*80}\")\n","    logf(f\"ITEM {index+1} - FIM DO PROCESSAMENTO\")\n","    logf(f\"{'='*80}\\n\")\n","\n","    return r\n","\n","# =========================\n","# Processar planilha (5 primeiros)\n","# =========================\n","\n","def processar_planilha_teste(df):\n","    n = min(LIMITE_TESTE, len(df))\n","    print(f\"\\n{'='*80}\")\n","    print(f\"[TESTE] Processando {n} primeiros produtos\")\n","    print(f\"[CONFIG] 5 prompts identifica√ß√£o √ó 3 modelos = 15 identifica√ß√µes/produto\")\n","    print(f\"[CONFIG] 5 prompts pre√ßo √ó 3 modelos = 15 pre√ßos/produto identificado\")\n","    print(f\"{'='*80}\\n\")\n","\n","    resultados = []\n","\n","    with tqdm(total=n, desc=\"[Produtos]\", position=0, dynamic_ncols=True) as bar:\n","        for idx in range(n):\n","            desc = df.iloc[idx].get('descricao_item_bruta', '')\n","\n","            if pd.isna(desc) or str(desc).strip() == '':\n","                print(f\"[SKIP] Item {idx+1}: sem descri√ß√£o\", flush=True)\n","                bar.update(1)\n","                continue\n","\n","            bar.set_postfix(dict(item=f\"{idx+1}/{n}\"))\n","\n","            resultado = processar_item_completo(desc, idx)\n","            resultados.append(resultado)\n","\n","            bar.update(1)\n","\n","    print(f\"\\n[OK] Processamento conclu√≠do!\", flush=True)\n","\n","    # Criar DataFrame com os resultados\n","    df_resultados = pd.DataFrame(resultados)\n","\n","    return df_resultados\n","\n","# =========================\n","# An√°lise e Relat√≥rios\n","# =========================\n","\n","def criar_analise_completa(df_resultados, pasta_drive):\n","    \"\"\"\n","    Cria an√°lises detalhadas e salva em abas separadas do Excel\n","    \"\"\"\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    destino = f\"/content/drive/{pasta_drive}/Resultado_Varios_Prompts_{ts}.xlsx\"\n","\n","    try:\n","        with pd.ExcelWriter(destino, engine='openpyxl') as writer:\n","            # ABA 1: Dados Completos\n","            df_resultados.to_excel(writer, sheet_name='Dados_Completos', index=False)\n","\n","            # ABA 2: An√°lise de Identifica√ß√£o\n","            analise_id = analisar_identificacao(df_resultados)\n","            analise_id.to_excel(writer, sheet_name='Analise_Identificacao', index=False)\n","\n","            # ABA 3: An√°lise de Pre√ßos\n","            analise_preco = analisar_precos(df_resultados)\n","            analise_preco.to_excel(writer, sheet_name='Analise_Precos', index=False)\n","\n","            # ABA 4: Ranking Geral\n","            ranking = criar_ranking_geral(df_resultados)\n","            ranking.to_excel(writer, sheet_name='Ranking_Prompts', index=False)\n","\n","            # ABA 5: Estat√≠sticas por Modelo\n","            stats_modelo = estatisticas_por_modelo(df_resultados)\n","            stats_modelo.to_excel(writer, sheet_name='Stats_por_Modelo', index=False)\n","\n","            # Ajustar largura das colunas em todas as abas\n","            for sheet_name in writer.sheets:\n","                worksheet = writer.sheets[sheet_name]\n","                for column in worksheet.columns:\n","                    max_length = 0\n","                    column_letter = column[0].column_letter\n","                    for cell in column:\n","                        try:\n","                            if len(str(cell.value)) > max_length:\n","                                max_length = len(str(cell.value))\n","                        except:\n","                            pass\n","                    adjusted_width = min(max_length + 2, 60)\n","                    worksheet.column_dimensions[column_letter].width = adjusted_width\n","\n","        print(f\"\\n[OK] Relat√≥rio completo salvo: {os.path.basename(destino)}\", flush=True)\n","        return destino\n","\n","    except Exception as e:\n","        print(f\"[ERRO] Falha ao criar an√°lise: {e}\", flush=True)\n","        return None\n","\n","def analisar_identificacao(df):\n","    \"\"\"Analisa taxa de sucesso na identifica√ß√£o de produtos\"\"\"\n","    resultados = []\n","\n","    for prompt_id in PROMPTS_IDENTIFICACAO.keys():\n","        for modelo in ['GPT4o', 'Claude', 'DeepSeek']:\n","            col = f'Produto_Identificado_{prompt_id}_{modelo}'\n","            if col in df.columns:\n","                total = len(df)\n","                sucesso = df[col].notna().sum()\n","                taxa = (sucesso / total * 100) if total > 0 else 0\n","\n","                resultados.append({\n","                    'Prompt_Identificacao': prompt_id,\n","                    'Modelo': modelo,\n","                    'Total_Itens': total,\n","                    'Identificacoes_Sucesso': sucesso,\n","                    'Taxa_Sucesso_%': round(taxa, 1)\n","                })\n","\n","    return pd.DataFrame(resultados)\n","\n","def analisar_precos(df):\n","    \"\"\"Analisa pre√ßos obtidos por cada combina√ß√£o\"\"\"\n","    resultados = []\n","\n","    for prompt_id in PROMPTS_IDENTIFICACAO.keys():\n","        for prompt_preco in PROMPTS_PRECO.keys():\n","            for modelo in ['GPT4o', 'Claude', 'DeepSeek']:\n","                col = f'Preco_{prompt_id}_{prompt_preco}_{modelo}'\n","                if col in df.columns:\n","                    precos = df[col].dropna()\n","                    if len(precos) > 0:\n","                        resultados.append({\n","                            'Prompt_Identificacao': prompt_id,\n","                            'Prompt_Preco': prompt_preco,\n","                            'Modelo': modelo,\n","                            'N_Precos': len(precos),\n","                            'Preco_Medio': round(precos.mean(), 2),\n","                            'Preco_Mediano': round(precos.median(), 2),\n","                            'Preco_Min': round(precos.min(), 2),\n","                            'Preco_Max': round(precos.max(), 2),\n","                            'Desvio_Padrao': round(precos.std(), 2)\n","                        })\n","\n","    return pd.DataFrame(resultados)\n","\n","def criar_ranking_geral(df):\n","    \"\"\"Cria ranking das melhores combina√ß√µes\"\"\"\n","    resultados = []\n","\n","    # Para cada combina√ß√£o de prompts\n","    for prompt_id in PROMPTS_IDENTIFICACAO.keys():\n","        for prompt_preco in PROMPTS_PRECO.keys():\n","            score_total = 0\n","            n_precos_total = 0\n","\n","            for modelo in ['GPT4o', 'Claude', 'DeepSeek']:\n","                col = f'Preco_{prompt_id}_{prompt_preco}_{modelo}'\n","                if col in df.columns:\n","                    n_precos = df[col].notna().sum()\n","                    n_precos_total += n_precos\n","                    score_total += n_precos\n","\n","            taxa = (n_precos_total / (len(df) * 3) * 100) if len(df) > 0 else 0\n","\n","            resultados.append({\n","                'Prompt_Identificacao': prompt_id,\n","                'Prompt_Preco': prompt_preco,\n","                'Total_Precos_Obtidos': n_precos_total,\n","                'Taxa_Sucesso_%': round(taxa, 1),\n","                'Score': score_total\n","            })\n","\n","    df_ranking = pd.DataFrame(resultados)\n","    df_ranking = df_ranking.sort_values('Score', ascending=False)\n","    df_ranking['Ranking'] = range(1, len(df_ranking) + 1)\n","\n","    return df_ranking[['Ranking', 'Prompt_Identificacao', 'Prompt_Preco',\n","                       'Total_Precos_Obtidos', 'Taxa_Sucesso_%', 'Score']]\n","\n","def estatisticas_por_modelo(df):\n","    \"\"\"Estat√≠sticas gerais por modelo\"\"\"\n","    resultados = []\n","\n","    for modelo in ['GPT4o', 'Claude', 'DeepSeek']:\n","        # Contar identifica√ß√µes\n","        cols_id = [col for col in df.columns if col.startswith('Produto_Identificado_') and col.endswith(f'_{modelo}')]\n","        total_id = sum(df[col].notna().sum() for col in cols_id)\n","\n","        # Contar pre√ßos\n","        cols_preco = [col for col in df.columns if col.startswith('Preco_') and col.endswith(f'_{modelo}')]\n","        total_precos = sum(df[col].notna().sum() for col in cols_preco)\n","\n","        # Calcular m√©dias de pre√ßo\n","        todos_precos = []\n","        for col in cols_preco:\n","            todos_precos.extend(df[col].dropna().tolist())\n","\n","        resultados.append({\n","            'Modelo': modelo,\n","            'Total_Identificacoes': total_id,\n","            'Total_Precos': total_precos,\n","            'Preco_Medio_Geral': round(np.mean(todos_precos), 2) if todos_precos else 0,\n","            'Preco_Mediano_Geral': round(np.median(todos_precos), 2) if todos_precos else 0\n","        })\n","\n","    return pd.DataFrame(resultados)\n","\n","def imprimir_resumo_console(df):\n","    \"\"\"Imprime resumo no console\"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"RESUMO DA AN√ÅLISE\")\n","    print(\"=\"*80)\n","\n","    print(f\"\\nüìä Total de produtos analisados: {len(df)}\")\n","\n","    # Total de colunas criadas\n","    cols_produto = [col for col in df.columns if col.startswith('Produto_Identificado_')]\n","    cols_preco = [col for col in df.columns if col.startswith('Preco_')]\n","\n","    print(f\"üìù Total de colunas de identifica√ß√£o: {len(cols_produto)}\")\n","    print(f\"üí∞ Total de colunas de pre√ßo: {len(cols_preco)}\")\n","\n","    # Taxa geral de sucesso\n","    total_id_esperadas = len(df) * len(PROMPTS_IDENTIFICACAO) * 3\n","    total_id_obtidas = sum(df[col].notna().sum() for col in cols_produto)\n","    taxa_id = (total_id_obtidas / total_id_esperadas * 100) if total_id_esperadas > 0 else 0\n","\n","    print(f\"\\n‚úÖ Taxa geral de identifica√ß√£o: {taxa_id:.1f}% ({total_id_obtidas}/{total_id_esperadas})\")\n","\n","    # Melhor modelo\n","    for modelo in ['GPT4o', 'Claude', 'DeepSeek']:\n","        cols_modelo = [col for col in cols_produto if col.endswith(f'_{modelo}')]\n","        sucesso = sum(df[col].notna().sum() for col in cols_modelo)\n","        print(f\"   - {modelo}: {sucesso} identifica√ß√µes\")\n","\n","    print(\"\\n\" + \"=\"*80 + \"\\n\")\n","\n","# =========================\n","# Execu√ß√£o Principal\n","# =========================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"LICITA+ - TESTE COM M√öLTIPLOS PROMPTS\")\n","print(\"=\"*80)\n","print(f\"Vers√£o: Teste com {LIMITE_TESTE} produtos\")\n","print(f\"Prompts de Identifica√ß√£o: {len(PROMPTS_IDENTIFICACAO)}\")\n","print(f\"Prompts de Pre√ßo: {len(PROMPTS_PRECO)}\")\n","print(f\"Modelos: 3 (GPT-4o-mini, Claude, DeepSeek)\")\n","print(\"=\"*80 + \"\\n\")\n","\n","# Carregar planilha\n","df_licitacoes, _ = carregar_planilha(NOME_ARQUIVO_EXCEL, PASTA_DRIVE)\n","\n","# Processar os 5 primeiros\n","df_resultados = processar_planilha_teste(df_licitacoes)\n","\n","# Criar an√°lise completa\n","caminho_relatorio = criar_analise_completa(df_resultados, PASTA_DRIVE)\n","\n","# Imprimir resumo\n","imprimir_resumo_console(df_resultados)\n","\n","# Salvar logs\n","if LOG_SALVAR_TXT:\n","    caminho_logs = salvar_logs_txt(PASTA_DRIVE)\n","\n","print(f\"\\n‚úÖ FINALIZADO COM SUCESSO!\")\n","print(f\"üìÅ Relat√≥rio: {os.path.basename(caminho_relatorio) if caminho_relatorio else 'erro ao salvar'}\")\n","print(\"\\n\")"],"metadata":{"id":"q7cyzZziPceU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Compara√ß√£o de Equival√™ncia de Produtos\n","# Avalia se produtos identificados por LLMs s√£o equivalentes aos produtos manuais\n","# =========================\n","\n","NOME_ARQUIVO_EXCEL = \"relatorio_final_20251016_023256.xlsx\"\n","PASTA_DRIVE = \"My Drive/üìÅ FACULDADE/2025 - 8¬∫ Per√≠odo/Trabalho de Conclus√£o de Curso II\"\n","\n","OPENAI_API_KEY = \"\"      # use Secrets (google.colab.userdata)\n","ANTHROPIC_API_KEY = \"\"   # use Secrets\n","DEEPSEEK_API_KEY = \"\"    # use Secrets\n","\n","# Configura√ß√µes\n","SLEEP_ENTRE_CHAMADAS = 1.0  # segundos\n","VERBOSO = True\n","LOG_SALVAR_TXT = True\n","LOG_REGISTROS = []\n","\n","# =========================\n","# Instala√ß√£o de depend√™ncias\n","# =========================\n","\n","import subprocess, sys\n","def instalar():\n","    deps = [\n","        \"openai>=1.3.0\",\n","        \"anthropic>=0.18.0\",\n","        \"pandas>=1.5.0\",\n","        \"openpyxl>=3.1.0\",\n","        \"numpy>=1.23.0\",\n","        \"tqdm>=4.66.0\"\n","    ]\n","    for dep in deps:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", dep, \"-q\"])\n","instalar()\n","\n","# =========================\n","# Imports\n","# =========================\n","\n","import pandas as pd\n","import numpy as np\n","import openai\n","import anthropic\n","import time\n","import os\n","from datetime import datetime\n","from tqdm.auto import tqdm\n","\n","# =========================\n","# Configura√ß√£o de APIs\n","# =========================\n","\n","from google.colab import userdata\n","\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY') or OPENAI_API_KEY\n","ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY') or ANTHROPIC_API_KEY\n","DEEPSEEK_API_KEY = userdata.get('DEEPSEEK_API_KEY') or DEEPSEEK_API_KEY\n","\n","openai.api_key = OPENAI_API_KEY\n","cliente_anthropic = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n","cliente_deepseek = openai.OpenAI(api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\")\n","\n","print(\"[OK] APIs configuradas\", flush=True)\n","\n","# =========================\n","# Google Drive\n","# =========================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","print(\"[OK] Drive montado\", flush=True)\n","\n","# =========================\n","# Utilit√°rios\n","# =========================\n","\n","def logf(msg):\n","    LOG_REGISTROS.append(msg)\n","    if VERBOSO:\n","        print(msg, flush=True)\n","\n","def salvar_logs_txt(pasta_drive):\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    caminho = f\"/content/drive/{pasta_drive}/logs_comparacao_{ts}.txt\"\n","    with open(caminho, \"w\", encoding=\"utf-8\") as f:\n","        f.write(\"\\n\".join(LOG_REGISTROS))\n","        f.write(\"\\n\")\n","    print(f\"[OK] Log salvo: {os.path.basename(caminho)}\", flush=True)\n","    return caminho\n","\n","# =========================\n","# Carregar Planilha\n","# =========================\n","\n","def carregar_planilha(nome_arquivo, pasta_drive):\n","    caminho = f\"/content/drive/{pasta_drive}/{nome_arquivo}\"\n","    try:\n","        df = pd.read_excel(caminho, engine='openpyxl')\n","        print(f\"[OK] Planilha carregada: {len(df)} linhas\", flush=True)\n","        print(f\"[INFO] Colunas dispon√≠veis: {list(df.columns)}\", flush=True)\n","        return df, caminho\n","    except Exception as e:\n","        print(f\"[ERRO] Falha ao carregar planilha: {e}\", flush=True)\n","        raise\n","\n","def salvar_relatorio_comparacao(df, pasta_drive):\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    destino = f\"/content/drive/{pasta_drive}/comparacao_equivalencia_{ts}.xlsx\"\n","    try:\n","        df.to_excel(destino, index=False, engine='openpyxl')\n","        print(f\"[OK] Relat√≥rio salvo: {os.path.basename(destino)}\", flush=True)\n","        return True, destino\n","    except Exception as e:\n","        print(f\"[ERRO] Falha ao salvar relat√≥rio: {e}\", flush=True)\n","        return False, None\n","\n","# =========================\n","# Fun√ß√µes de Consulta LLM\n","# =========================\n","\n","def consultar_gpt4o(prompt, max_tokens=50, temperatura=0.0):\n","    for tent in range(1, 4):\n","        try:\n","            rsp = openai.chat.completions.create(\n","                model=\"gpt-4o-mini\",\n","                messages=[\n","                    {\"role\": \"system\", \"content\": \"Voc√™ √© um especialista em compara√ß√£o de produtos. Responda apenas SIM ou N√ÉO.\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                max_tokens=max_tokens,\n","                temperature=temperatura\n","            )\n","            return rsp.choices[0].message.content.strip()\n","        except Exception as e:\n","            logf(f\"[WARN] GPT tentativa {tent} falhou: {str(e)[:100]}\")\n","            time.sleep(3)\n","    return None\n","\n","def consultar_claude(prompt, max_tokens=50, temperatura=0.0):\n","    for tent in range(1, 4):\n","        try:\n","            msg = cliente_anthropic.messages.create(\n","                model=\"claude-sonnet-4-5-20250929\",\n","                max_tokens=max_tokens,\n","                temperature=temperatura,\n","                messages=[{\"role\": \"user\", \"content\": prompt}]\n","            )\n","            return msg.content[0].text.strip()\n","        except Exception as e:\n","            logf(f\"[WARN] Claude tentativa {tent} falhou: {str(e)[:100]}\")\n","            time.sleep(3)\n","    return None\n","\n","def consultar_deepseek(prompt, max_tokens=50, temperatura=0.0):\n","    for tent in range(1, 4):\n","        try:\n","            rsp = cliente_deepseek.chat.completions.create(\n","                model=\"deepseek-chat\",\n","                messages=[\n","                    {\"role\": \"system\", \"content\": \"You are a product comparison expert. Answer only YES or NO.\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                max_tokens=max_tokens,\n","                temperature=temperatura\n","            )\n","            return rsp.choices[0].message.content.strip()\n","        except Exception as e:\n","            s = str(e)\n","            if \"Insufficient Balance\" in s or \"402\" in s:\n","                logf(f\"[WARN] DeepSeek sem saldo (tentativa {tent})\")\n","                return None\n","            logf(f\"[WARN] DeepSeek tentativa {tent} falhou: {s[:100]}\")\n","            time.sleep(3)\n","    return None\n","\n","# =========================\n","# Prompt de Compara√ß√£o\n","# =========================\n","\n","def criar_prompt_comparacao(produto_llm, produto_manual):\n","    \"\"\"Cria o prompt para comparar dois produtos\"\"\"\n","    prompt = f\"\"\"Compare os seguintes produtos e determine se s√£o EQUIVALENTES:\n","\n","Produto 1 (identificado por LLM): {produto_llm}\n","Produto 2 (identificado manualmente): {produto_manual}\n","\n","Os produtos s√£o considerados EQUIVALENTES se:\n","- Possuem as mesmas caracter√≠sticas essenciais\n","- T√™m a mesma funcionalidade/aplica√ß√£o\n","- S√£o da mesma categoria/tipo\n","- Possuem qualidade similar\n","\n","IMPORTANTE: N√£o precisam ser EXATAMENTE o mesmo produto ou mesma marca, apenas equivalentes em fun√ß√£o e qualidade.\n","\n","Responda APENAS com uma das op√ß√µes:\n","- SIM (se s√£o equivalentes)\n","- N√ÉO (se n√£o s√£o equivalentes)\n","- INCONCLUSIVO (se n√£o h√° informa√ß√£o suficiente para decidir)\n","\n","Resposta:\"\"\"\n","    return prompt\n","\n","# =========================\n","# Normalizar Resposta\n","# =========================\n","\n","def normalizar_resposta(texto):\n","    \"\"\"Normaliza a resposta do LLM para SIM, N√ÉO ou INCONCLUSIVO\"\"\"\n","    if not texto:\n","        return \"INCONCLUSIVO\"\n","\n","    texto = texto.upper().strip()\n","\n","    # Verifica respostas diretas\n","    if \"SIM\" in texto or \"YES\" in texto or \"EQUIVALENTE\" in texto:\n","        return \"SIM\"\n","    elif \"N√ÉO\" in texto or \"NAO\" in texto or \"NO\" in texto or \"NOT\" in texto:\n","        return \"N√ÉO\"\n","    else:\n","        return \"INCONCLUSIVO\"\n","\n","# =========================\n","# Comparar Produtos\n","# =========================\n","\n","def comparar_produtos_completo(row, index):\n","    \"\"\"\n","    Compara cada produto LLM com cada produto manual usando os 3 LLMs\n","    Total: 9 compara√ß√µes (3 produtos LLM √ó 3 produtos manuais)\n","    \"\"\"\n","    resultados = {}\n","\n","    # Produtos identificados pelos LLMs\n","    produtos_llm = {\n","        'GPT': row.get('produto_identificado_modelo_01', None),\n","        'Claude': row.get('produto_identificado_modelo_02', None),\n","        'DeepSeek': row.get('produto_identificado_modelo_03', None)\n","    }\n","\n","    # Produtos manuais (os nomes dos produtos, n√£o os pre√ßos!)\n","    produtos_manuais = {\n","        'Manual1': row.get('fonte_preco_ref1', None),\n","        'Manual2': row.get('fonte_preco_ref2', None),\n","        'Manual3': row.get('fonte_preco_ref3', None)\n","    }\n","\n","    logf(f\"\\n{'='*80}\")\n","    logf(f\"ITEM {index + 1}\")\n","    logf(f\"{'='*80}\")\n","\n","    # Mapear fun√ß√µes LLM\n","    llms_funcoes = {\n","        'GPT': consultar_gpt4o,\n","        'Claude': consultar_claude,\n","        'DeepSeek': consultar_deepseek\n","    }\n","\n","    comparacoes_realizadas = 0\n","    total_comparacoes = 0\n","\n","    # Para cada produto identificado pelo LLM\n","    for nome_llm, produto_llm in produtos_llm.items():\n","        if not produto_llm or pd.isna(produto_llm):\n","            logf(f\"[SKIP] {nome_llm}: produto n√£o identificado\")\n","            continue\n","\n","        # Para cada produto manual\n","        for nome_manual, produto_manual in produtos_manuais.items():\n","            if not produto_manual or pd.isna(produto_manual):\n","                continue\n","\n","            logf(f\"\\n--- Comparando {nome_llm} √ó {nome_manual} ---\")\n","            logf(f\"Produto LLM ({nome_llm}): {produto_llm}\")\n","            logf(f\"Produto Manual ({nome_manual}): {produto_manual}\")\n","\n","            # Usar cada um dos 3 LLMs para fazer a compara√ß√£o\n","            for nome_llm_avaliador, func_llm in llms_funcoes.items():\n","                total_comparacoes += 1\n","                col_name = f\"comp_{nome_llm}_vs_{nome_manual}_{nome_llm_avaliador}\"\n","\n","                try:\n","                    prompt = criar_prompt_comparacao(produto_llm, produto_manual)\n","                    resposta = func_llm(prompt, max_tokens=50, temperatura=0.0)\n","                    resposta_normalizada = normalizar_resposta(resposta)\n","\n","                    resultados[col_name] = resposta_normalizada\n","                    comparacoes_realizadas += 1\n","\n","                    logf(f\"  [{nome_llm_avaliador}] Resposta: {resposta_normalizada} (raw: {resposta})\")\n","\n","                    time.sleep(SLEEP_ENTRE_CHAMADAS)\n","\n","                except Exception as e:\n","                    resultados[col_name] = \"ERRO\"\n","                    logf(f\"  [{nome_llm_avaliador}] ERRO: {str(e)[:100]}\")\n","\n","    # Calcular estat√≠sticas\n","    resultados['total_comparacoes_realizadas'] = comparacoes_realizadas\n","    resultados['total_comparacoes_possiveis'] = total_comparacoes\n","\n","    # Contar respostas\n","    valores = [v for v in resultados.values() if isinstance(v, str) and v in ['SIM', 'N√ÉO', 'INCONCLUSIVO']]\n","    resultados['total_sim'] = valores.count('SIM')\n","    resultados['total_nao'] = valores.count('N√ÉO')\n","    resultados['total_inconclusivo'] = valores.count('INCONCLUSIVO')\n","\n","    if valores:\n","        resultados['percentual_equivalencia'] = round((resultados['total_sim'] / len(valores)) * 100, 2)\n","    else:\n","        resultados['percentual_equivalencia'] = None\n","\n","    logf(f\"\\n[RESUMO ITEM {index + 1}]\")\n","    logf(f\"Compara√ß√µes realizadas: {comparacoes_realizadas}/{total_comparacoes}\")\n","    logf(f\"SIM: {resultados['total_sim']} | N√ÉO: {resultados['total_nao']} | INCONCLUSIVO: {resultados['total_inconclusivo']}\")\n","    logf(f\"Percentual de equival√™ncia: {resultados['percentual_equivalencia']}%\")\n","\n","    return resultados\n","\n","# =========================\n","# Processar Planilha\n","# =========================\n","\n","def processar_planilha_comparacao(df, limite_linhas=None):\n","    \"\"\"Processa a planilha fazendo as compara√ß√µes\"\"\"\n","    n = limite_linhas if limite_linhas else len(df)\n","    n = min(n, len(df))\n","\n","    print(f\"\\n[INFO] Iniciando compara√ß√£o de equival√™ncia\", flush=True)\n","    print(f\"[INFO] Total de itens: {n}\", flush=True)\n","    print(f\"[INFO] Compara√ß√µes por item: at√© 9 (3 LLMs identificadores √ó 3 produtos manuais √ó 1 LLM avaliador)\", flush=True)\n","    print(f\"[INFO] Na verdade s√£o 27 compara√ß√µes (3 LLMs √ó 3 produtos manuais √ó 3 LLMs avaliadores)\", flush=True)\n","\n","    # Criar colunas para resultados\n","    # Formato: comp_{LLM_identificador}_vs_{Manual}__{LLM_avaliador}\n","    nomes_llm = ['GPT', 'Claude', 'DeepSeek']\n","    nomes_manual = ['Manual1', 'Manual2', 'Manual3']\n","\n","    for llm in nomes_llm:\n","        for manual in nomes_manual:\n","            for avaliador in nomes_llm:\n","                col_name = f\"comp_{llm}_vs_{manual}_{avaliador}\"\n","                if col_name not in df.columns:\n","                    df[col_name] = None\n","\n","    # Colunas de estat√≠sticas\n","    colunas_stats = [\n","        'total_comparacoes_realizadas',\n","        'total_comparacoes_possiveis',\n","        'total_sim',\n","        'total_nao',\n","        'total_inconclusivo',\n","        'percentual_equivalencia'\n","    ]\n","    for col in colunas_stats:\n","        if col not in df.columns:\n","            df[col] = None\n","\n","    # Processar cada linha\n","    with tqdm(total=n, desc=\"[Comparando]\", position=0, dynamic_ncols=True) as pbar:\n","        for idx in range(n):\n","            pbar.set_postfix(dict(item=f\"{idx+1}/{n}\"))\n","\n","            resultados = comparar_produtos_completo(df.iloc[idx], idx)\n","\n","            # Atualizar DataFrame\n","            for col, valor in resultados.items():\n","                df.at[idx, col] = valor\n","\n","            pbar.update(1)\n","\n","    print(f\"\\n[OK] Compara√ß√£o conclu√≠da!\", flush=True)\n","    return df\n","\n","# =========================\n","# Gerar Relat√≥rio Estat√≠stico\n","# =========================\n","\n","def gerar_relatorio_estatistico(df):\n","    \"\"\"Gera relat√≥rio com estat√≠sticas das compara√ß√µes\"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"RELAT√ìRIO DE COMPARA√á√ÉO DE EQUIVAL√äNCIA\")\n","    print(\"=\"*80)\n","\n","    # Filtrar apenas linhas processadas\n","    df_proc = df[df['total_comparacoes_realizadas'].notna()]\n","\n","    if len(df_proc) == 0:\n","        print(\"[INFO] Nenhuma compara√ß√£o realizada\")\n","        return\n","\n","    print(f\"\\nItens processados: {len(df_proc)}\")\n","    print(f\"Total de compara√ß√µes realizadas: {df_proc['total_comparacoes_realizadas'].sum():.0f}\")\n","\n","    print(\"\\n--- Distribui√ß√£o de Respostas ---\")\n","    print(f\"SIM (equivalentes): {df_proc['total_sim'].sum():.0f}\")\n","    print(f\"N√ÉO (n√£o equivalentes): {df_proc['total_nao'].sum():.0f}\")\n","    print(f\"INCONCLUSIVO: {df_proc['total_inconclusivo'].sum():.0f}\")\n","\n","    print(\"\\n--- Percentual de Equival√™ncia ---\")\n","    perc_equiv = df_proc['percentual_equivalencia'].dropna()\n","    if len(perc_equiv) > 0:\n","        print(f\"M√©dia: {perc_equiv.mean():.2f}%\")\n","        print(f\"Mediana: {perc_equiv.median():.2f}%\")\n","        print(f\"Desvio padr√£o: {perc_equiv.std():.2f}%\")\n","        print(f\"M√≠nimo: {perc_equiv.min():.2f}%\")\n","        print(f\"M√°ximo: {perc_equiv.max():.2f}%\")\n","\n","    # An√°lise por LLM identificador\n","    print(\"\\n--- Equival√™ncia por LLM Identificador ---\")\n","    nomes_llm = ['GPT', 'Claude', 'DeepSeek']\n","    for llm in nomes_llm:\n","        # Pegar todas as colunas de compara√ß√£o deste LLM\n","        cols_llm = [col for col in df.columns if col.startswith(f\"comp_{llm}_vs_\")]\n","        if cols_llm:\n","            # Contar SIM para este LLM\n","            total_sim = 0\n","            total_respostas = 0\n","            for col in cols_llm:\n","                valores = df_proc[col].dropna()\n","                total_sim += (valores == 'SIM').sum()\n","                total_respostas += len(valores[valores.isin(['SIM', 'N√ÉO', 'INCONCLUSIVO'])])\n","\n","            if total_respostas > 0:\n","                perc = (total_sim / total_respostas) * 100\n","                print(f\"{llm}: {perc:.2f}% equival√™ncia ({total_sim}/{total_respostas})\")\n","\n","    print(\"\\n\" + \"=\"*80)\n","\n","# =========================\n","# Execu√ß√£o Principal\n","# =========================\n","\n","print(\"\\n[START] Compara√ß√£o de Equival√™ncia de Produtos\", flush=True)\n","print(\"[INFO] Este script compara produtos identificados por LLMs com produtos manuais\", flush=True)\n","\n","# Carregar planilha\n","df_original, _ = carregar_planilha(NOME_ARQUIVO_EXCEL, PASTA_DRIVE)\n","\n","# Processar compara√ß√µes\n","df_resultado = processar_planilha_comparacao(\n","    df_original.copy(),\n","    limite_linhas=None  # None para processar tudo\n",")\n","\n","# Salvar resultado\n","ok, caminho_resultado = salvar_relatorio_comparacao(df_resultado, PASTA_DRIVE)\n","\n","# Gerar relat√≥rio estat√≠stico\n","if ok:\n","    gerar_relatorio_estatistico(df_resultado)\n","\n","# Salvar logs\n","if LOG_SALVAR_TXT:\n","    caminho_logs = salvar_logs_txt(PASTA_DRIVE)\n","\n","print(\"\\n[END] Processamento conclu√≠do!\", flush=True)"],"metadata":{"id":"X761rlmpQHAd"},"execution_count":null,"outputs":[]}]}